{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f29c7c-1049-45f8-b18e-ade62a4fff52",
   "metadata": {},
   "source": [
    "#### <font color = 'green'> Emotion Sentiment Analysis with NLP model and Industrialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d54667e-a7f8-4cde-96f8-66900db931ac",
   "metadata": {},
   "source": [
    "###### Import necessary libaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d932e0b-f0a5-4844-b131-2baf697d4330",
   "metadata": {},
   "source": [
    "###### pip install if not already installed"
   ]
  },
  {
   "cell_type": "raw",
   "id": "987e1947-c47c-4980-9b5d-e8d12a9f39f6",
   "metadata": {},
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d544d60a-09f3-4d49-8f73-a9dfed4d5255",
   "metadata": {},
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a429101e-7c28-4bd1-aea1-bc61d0bcfa4a",
   "metadata": {},
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f9b3de5-d3a8-4d80-895e-b614a92d7040",
   "metadata": {},
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82d074b3-2796-4dc6-9c7c-cdab82b6b484",
   "metadata": {},
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3846157-5d9c-43f3-a49b-e139f37e7b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Olanrewaju\n",
      "[nltk_data]     Adegoke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Olanrewaju\n",
      "[nltk_data]     Adegoke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import pickle\n",
    "import contractions\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, GRU, Flatten,Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f516b9b3-265e-4c29-bbce-db9e65157d97",
   "metadata": {},
   "source": [
    "###### Define the path to the folders containing the preprocessing and model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96fca86f-1030-44dd-94cb-d6b882b98453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Olanrewaju Adegoke\\\\Desktop\\\\TechTern\\\\nlp_projects\\\\nlp_project_sentiment_analysis\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30708e36-36f8-447e-9294-3df52b055770",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data'\n",
    "model_path = '../models'\n",
    "note_path = '../notebooks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704c79e-d20d-492a-ab3d-0ae6a8811650",
   "metadata": {},
   "source": [
    "###### Function for cleaning the `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dac9ea6-b078-47cd-9f01-27c15edc7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower(features):\n",
    "    lower_case_text = features.lower()\n",
    "    return lower_case_text \n",
    "\n",
    "def remove_contractions(features):\n",
    "    text_without_contraction = contractions.fix(features)\n",
    "    return text_without_contraction\n",
    "\n",
    "def remove_punctuation(features):\n",
    "    string_punc = string.punctuation\n",
    "    text_without_punc = \"\".join([word for word in features if word not in string_punc])\n",
    "    return text_without_punc\n",
    "\n",
    "def remove_number(features):\n",
    "    num_removed = ''.join([char for char in features if not char.isdigit()])\n",
    "    return num_removed\n",
    "\n",
    "def remove_symbols(features):\n",
    "    word_list = []\n",
    "    words = features.split()\n",
    "    for word in words:\n",
    "        no_symbol = ''.join([char for char in word if char.isalpha()])\n",
    "        word_list.append(no_symbol)\n",
    "        text_no_sym = ' '.join([word for word in word_list])   \n",
    "    return text_no_sym\n",
    "\n",
    "def remove_stopwords(features):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = features.split()\n",
    "    no_stop_word = ' '.join([word for word in words if word not in stop_words])\n",
    "    return no_stop_word\n",
    "\n",
    "def stemming(features):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [token for token in features.split()]\n",
    "    stemmed_word = ' '.join([stemmer.stem(word) for word in tokens])\n",
    "    return stemmed_word \n",
    "\n",
    "def lemmatization(features):\n",
    "    lemmer =  WordNetLemmatizer()\n",
    "    tokens = [token for token in features.split()]\n",
    "    lemmed_word = ' '.join([lemmer.lemmatize(word) for word in tokens])\n",
    "    return lemmed_word "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfa270b-8576-4615-8ec8-adaebfde0ec3",
   "metadata": {},
   "source": [
    "###### Preprocessing function that automate cleaning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "430c1d2b-9947-4155-aa13-8dffda1c99c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessor(features, mode=True):\n",
    "    '''\n",
    "    This function preprocessing a text data with the following sequence:\n",
    "    convert to lower\n",
    "    remove contractions\n",
    "    remove punctuations\n",
    "    remove numbers within text\n",
    "    remove alpha-numeric symbols\n",
    "    remove stopwords\n",
    "    get the root word by stemming if mode = True or lemmatization if mode = False.\n",
    "    Returns a cleaned text.\n",
    "\n",
    "    ***Developed by Olanrewaju Adegoke***\n",
    "    '''\n",
    "    features = convert_to_lower(features)\n",
    "    features = remove_contractions(features)\n",
    "    features = remove_punctuation(features)\n",
    "    features = remove_number(features)\n",
    "    features = remove_symbols(features)\n",
    "    features = remove_stopwords(features)\n",
    "    if mode == True:\n",
    "        features = stemming(features)\n",
    "    else:\n",
    "        features = lemmatization(features)\n",
    "    return features   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc99cc8e-33c9-4444-8047-39f32bbaf8c8",
   "metadata": {},
   "source": [
    "###### Function that load the model and preprocessing artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8098cb5e-fdcb-46a6-830e-04643e6771c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_tokenizer_artifact():\n",
    "    os.chdir(model_path)\n",
    "    with open('feature_tokenizer_and_label_encoder.pkl', 'rb') as file:\n",
    "        token_label = pickle.load(file)\n",
    "    return token_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac235a8-b5d2-401b-800d-fc431f1172b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_and_model_artifact():\n",
    "    os.chdir(model_path)\n",
    "    with open('keras_models_with_tokenization.pkl', 'rb') as file:\n",
    "        tokenizer_model = pickle.load(file)\n",
    "    return tokenizer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61491cc-a632-4571-b44f-4739b4835b82",
   "metadata": {},
   "source": [
    "###### Instantiate all the instances of the artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5f625ab-2f4d-491f-a82b-820b43c0665e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olanrewaju Adegoke\\Desktop\\TechTern\\nlp_projects\\nlp\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 9 variables whereas the saved optimizer has 16 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\Olanrewaju Adegoke\\Desktop\\TechTern\\nlp_projects\\nlp\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 13 variables whereas the saved optimizer has 24 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "os.chdir(model_path)\n",
    "tokenizer_model = load_tokenizer_and_model_artifact()\n",
    "token_label = load_feature_tokenizer_artifact()\n",
    "os.chdir(note_path)\n",
    "best_model_tokenized = tokenizer_model['RNN(LSTM_IMPROVED)']\n",
    "tokenizer = token_label['feature_tokenizer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8acd5-d8a6-4a25-8304-55646d1848ff",
   "metadata": {},
   "source": [
    "###### The prediction and inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a69f838c-0869-4921-8ee3-947fca1edaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "best_model_tokenized\n",
    "tokenizer\n",
    "maxlen_pad = 19\n",
    "\n",
    "def model_industrialization_tokenizer(text, padding=True):\n",
    "    \n",
    "    labels = ['anger', 'fear', 'joy', 'love', 'sadness', 'surprise']\n",
    "    \n",
    "    text = text_preprocessor(features=text, mode=True)\n",
    "    \n",
    "    # convert string of text to list\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "\n",
    "    test_feat_seq = tokenizer.texts_to_sequences(text)\n",
    "    \n",
    "    if padding == True:\n",
    "        padded_test_seq = pad_sequences(test_feat_seq, maxlen=maxlen_pad, padding='post')\n",
    "    else:\n",
    "        padded_test_seq = pad_sequences(test_feat_seq, maxlen=maxlen_pad, padding='pre')\n",
    "\n",
    "\n",
    "    predictions = best_model_tokenized.predict(padded_test_seq)\n",
    "\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "    predicted_label = labels[predicted_class]\n",
    "\n",
    "    max_prediction = np.max(predictions[0])\n",
    "\n",
    "    #pred_df = pd.DataFrame({'predicted_emotions': [max_prediction], 'emotions': [predicted_label]}, columns=['predicted_emotions', 'emotions'])\n",
    "    pred_df = pd.DataFrame({'emotions': [predicted_label]}, columns=['emotions'])\n",
    "\n",
    "    return pred_df  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f9139d-1645-4ce5-80f9-c8db0700115e",
   "metadata": {},
   "source": [
    "###### Install gradio GUI interface"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ac4b7c4-766e-4581-8430-004dc3d83d77",
   "metadata": {},
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877ecf37-ecbf-48bd-9fec-4141592331bd",
   "metadata": {},
   "source": [
    "###### Launch the gradio interface for industrializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e79114e8-4463-4374-9edd-7be70210d1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://d67bf1b77668688166.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d67bf1b77668688166.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "examples=[\n",
    "        [\"I'm so happy about my new job!\"],\n",
    "        [\"I'm feeling really sad today.\"],\n",
    "        [\"This makes me so angry!\"],\n",
    "        [\"I'm terrified of what might happen.\"],\n",
    "        [\"I love spending time with my family.\"],\n",
    "        [\"Wow, I didn't expect that at all!\"]]\n",
    "\n",
    "\n",
    "inputs=[\n",
    "        gr.Textbox(placeholder=\"What is on your mind...\") \n",
    "    ]\n",
    "\n",
    "outputs = [gr.Dataframe(row_count = (1, \"dynamic\"), col_count=(1, \"fixed\"), label='Thoughts and Emotions', headers=['Emotion'])]\n",
    "\n",
    "title='Thoughts and Emotions NLP model using Tokenization with TensorFlow by Olanrewaju Adegoke'\n",
    "description='Express what is on your mind and I will predict your present state of emotions.'\n",
    "\n",
    "interface = gr.Interface(fn = model_industrialization_tokenizer, inputs = inputs, outputs = outputs, title=title, description=description, examples=examples)\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb17ea0-d9a6-496f-bdba-d8f8e54e4ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
